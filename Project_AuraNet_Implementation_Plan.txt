Project: AuraNet - A Dual-Stream Forensic Network for Face Manipulation Detection

Objective: Implement a state-of-the-art, dual-stream deep learning model for face manipulation detection. The model must be configurable and will produce two outputs: a grayscale segmentation mask representing the intensity of fakeness and a binary classification label (real/fake).

This document provides the complete implementation plan. Please adhere to the specified architectures, modules, and training strategies. All necessary algorithms, parameters, and architectural details are embedded within this prompt.

Part 0: Global Configuration
You will implement the model such that the following parameters are configurable (e.g., via a YAML file). The default values are provided.

Image Size: img_size = 256x256

Backbone Stages: num_stages = 4 (These are Stages 2, 3, 4, 5 of the main backbone)

Backbone Depths: depths = [2, 2, 6, 2] (Blocks per stage)

Backbone Dims: dims = [64, 128, 256, 512] (Channels per stage)

HAFT Levels per Stage: num_haft_levels = [3, 3, 2, 1] (Hierarchical levels for HAFT in each of the 4 stages)

HAFT Radial Bins: num_radial_bins = 16

HAFT Context Dim: context_vector_dim = 64

Final Fusion Dim: dsf_dim = 512

Decoder Embed Dim: decoder_embed_dim = 342

Decoder Depth: decoder_depth = 1

Number of Classes: num_classes = 2

Part 1: Model Architecture Implementation
The model is composed of a unique initial processing stage, a 4-stage dual-stream backbone, a final fusion module, and two output heads.

1.1. Initial Processing (Stage 1)
This stage processes the raw input image to generate the initial H/4 x W/4 feature maps for the main backbone.

1.1.1. MSAF (Multi-Scale Artifact Fusion) Module

Purpose: To generate the initial features for the Frequency Stream by fusing high-resolution SRM features and mid-resolution DWT features.

Inputs:

rgb_image (B, 3, H, W)

Internal Process:

Convert rgb_image to grayscale (B, 1, H, W).

SRM Features: Apply the 10 fixed SRM filters (see Appendix A.1) to the grayscale image to produce srm_features (B, 10, H, W).

DWT Features: Apply a 2-level Discrete Wavelet Transform (using 'Haar' and 'coif1' wavelets). Extract and concatenate the LH, HH, and HL detail coefficients from both levels to produce dwt_features (B, 6, H/2, W/2).

SRM Path: A nn.Sequential block with Conv2d(10, 20, kernel_size=3, stride=2, padding=1), BatchNorm2d(20), GELU(). This processes srm_features.

DWT Path: A nn.Sequential block with Conv2d(6, 12, kernel_size=1), BatchNorm2d(12), GELU(). This processes dwt_features.

Concatenate the outputs of both paths to get fused_artifacts (B, 32, H/2, W/2).

CBAM-style Attention:

Generate a channel_attention map and a spatial_attention map from fused_artifacts.

Apply them sequentially: output = fused_artifacts * channel_attention, then output = output * spatial_attention.

Output: initial_freq_features_H2 (B, 32, H/2, W/2).

1.1.2. MBConv Downsample Block

Purpose: To process the initial artifact features and downsample them for the main backbone.

Input: initial_freq_features_H2 (B, 32, H/2, W/2).

Architecture:

Pointwise Expand: Conv2d(32, 128, kernel_size=1).

Depthwise Downsample: Conv2d(128, 128, kernel_size=3, stride=2, padding=1, groups=128), followed by LayerNorm and GELU.

SE Block: A standard Squeeze-and-Excitation block.

Pointwise Project: Conv2d(128, 64, kernel_size=1).

Residual Connection: Create a skip path by applying AvgPool2d(kernel_size=2, stride=2) and Conv2d(32, 64, kernel_size=1) to the block's input. Add this to the output of the projection.

Apply a final LayerNorm and GELU.

Output: freq_features_stage2_in (B, 64, H/4, W/4).

1.1.3. Initial Spatial Stem

Purpose: To generate the initial features for the Spatial Stream.

Input: rgb_image (B, 3, H, W).

Architecture: A nn.Sequential block containing:

Conv2d(3, 32, kernel_size=3, stride=2, padding=1), BatchNorm2d(32), GELU().

A downsampling block (e.g., Conv2d(32, 64, kernel_size=2, stride=2)) to match the output shape of the MBConv block.

Output: spatial_features_stage2_in (B, 64, H/4, W/4).

1.2. Dual-Stream Backbone (Stages 2-5)
This backbone consists of four stages. Each stage contains a Spatial Stream block, a Frequency Stream block, a Cross-Fusion block, and a Downsample layer.

1.2.1. Spatial Stream

Architecture: At each stage i, this stream is a nn.Sequential stack of depths[i] standard ConvNeXt V2 Block modules. The Block implementation is provided in Appendix A.3.

1.2.2. Frequency Stream: HAFT (Hierarchical Adaptive Frequency Transform) Block

Purpose: A sophisticated frequency-domain processing block that replaces a standard convolutional block.

Input: F_in (feature map from the previous stage).

Internal Components:

FrequencyContextEncoder: A dictionary of small CNN heads. Each head is for a specific level (0 to num_haft_levels-1) and type (mag/phase). Each head takes a single-channel 2D patch and outputs a fixed-size context vector (context_vector_dim).

FilterPredictor: Contains two separate MLPs, mag_mlp_head and phase_mlp_head, to predict filter profiles.

level_embedding: An nn.Embedding(max(num_haft_levels), 2 * context_vector_dim) for positional encoding.

Forward Pass Logic:

Multi-Level Context Extraction: Loop from level m=0 to num_haft_levels[stage_idx]-1. Divide F_in into (2**m)**2 patches. For each patch, compute its FFT, decompose into magnitude and phase, and pass each through the corresponding FrequencyContextEncoder head to get (cv_mag, cv_phase). Concatenate them to get cv_patch.

Hierarchical Filtering: For each patch at the deepest level:
a.  Gather its ancestral cv_patch vectors.
b.  Add the learnable level_embedding to each ancestral vector.
c.  Concatenate the enriched vectors to form fused_context_vector.
d.  Split fused_context_vector into fused_mag_vector and fused_phase_vector.
e.  Pass these to the FilterPredictor to get w_mag_profile and w_phase_profile.
f.  Reconstruct the full 2D radial filters from these 1D profiles using the Chebyshev distance method (see Appendix A.2).
g.  Apply the filters to the patch's magnitude and phase, recombine, and perform an inverse FFT. The order of elements in the feature map must be preserved after this operation.

Reconstruction: Use nn.Fold to reassemble the enhanced patches into the final output feature map F_out.

Output: An enhanced feature map F_out.

1.2.3. Inter-Stage Fusion: CrossFusionBlock

Purpose: To perform a bidirectional dialogue between the two streams after their respective processing within a stage.

Hybrid Attention Strategy:

For high-resolution stages (2 & 3), use Deformable Cross-Attention.

For low-resolution stages (4 & 5), use Standard (Global) Cross-Attention.

Architecture:

Contains two core attention modules: spatial_enhancer and frequency_enhancer, conditionally instantiated as DeformableCrossAttention or StandardCrossAttention.

The forward pass performs a bidirectional dialogue: enhanced_spatial = spatial_enhancer(query=spatial, kv=freq) and enhanced_freq = frequency_enhancer(query=freq, kv=spatial).

Each path then completes a full Transformer block sequence: Residual -> Norm -> ConvFFN -> Residual -> Norm. The ConvFFN is a 1x1 Conv -> GELU -> 1x1 Conv block.

Output: The two enhanced feature maps for the next stage.

Detailed Implementation: See Appendix B for a detailed breakdown of the DeformableCrossAttention and StandardCrossAttention modules.

1.2.4. DownsampleLayer

Purpose: Placed between each stage to reduce resolution.

Architecture: A nn.Sequential block containing LayerNorm and a nn.Conv2d with kernel_size=2, stride=2.

1.3. Final Fusion and Output Heads
1.3.1. DSF (Dynamic Selection Fusion) Module

Inputs: The final spatial_feat and freq_feat from Stage 5.

Architecture:

Combine: combined_feat = spatial_feat + freq_feat.

Dynamic Selection: Use AdaptiveAvgPool2d on combined_feat and an FC layer with GELU to predict softmax weights w_spatial and w_freq. Compute the weighted sum: selected_feat = (w_spatial * spatial_feat) + (w_freq * freq_feat).

Channel Attention: Apply a standard CBAM-style channel attention mechanism to selected_feat to get fused_output.

Outputs: fused_output and pooled_output (AdaptiveAvgPool2d of fused_output).

1.3.2. ClassificationHead

Input: pooled_output.

Architecture: An MLP (Linear -> ReLU -> Dropout(0.15) -> Linear) that outputs num_classes logits.

1.3.3. SegmentationHead

Input: fused_output.

Architecture (based on fcmae.py decoder):

A 1x1 Conv (proj) to project input channels to decoder_embed_dim.

A shallow stack (decoder_depth) of standard ConvNeXt V2 Blocks.

A final 1x1 Conv (pred) to project to a single output channel.

A final nn.Sigmoid activation.

Output: The final grayscale segmentation mask.

Part 2: Training Strategy
2.1. Stage 1: Self-Supervised Pre-training
Goal: Learn robust, task-relevant features from unlabeled data.

Architecture: The dual-stream encoder, two lightweight decoders (architecturally identical to the SegmentationHead), and a contrastive projection head.

Process:

Apply a high-ratio random mask to an input image.

Pass the masked image through the encoder.

Image Decoder reconstructs original pixels from the spatial_feat_map.

Mask Decoder reconstructs the ground-truth mask from the frequency_feat_map.

Projection Head generates an L2-normalized embedding from the pooled and concatenated final features.

Combined Loss (L_pretrain):

L_image: L2 loss between reconstructed and original pixels (on masked patches only), as per fcmae.py's forward_loss.

L_mask: 0.85 * L1Loss + 0.15 * SSIM_Loss between predicted and ground-truth masks.

L_supcon: Supervised Contrastive Loss on the embeddings. Use the implementation from the pytorch-metric-learning library.

2.2. Stage 2: Supervised Fine-tuning
Goal: Adapt the pre-trained model to the final tasks.

Architecture: Load the pre-trained encoder weights. Discard the pre-training decoders and projection head. Attach the final, randomly initialized DSF_Module, ClassificationHead, and SegmentationHead.

Combined Loss (L_finetune):

L_seg: The same 0.85 * L1Loss + 0.15 * SSIM_Loss.

L_class: Use an AM-Softmax Loss layer.

Optimizer: Use differential learning rates: a small LR for the encoder and a larger LR for the new heads.

Appendix A: Embedded Resources
A.1. 10 SRM Filter Kernels
These should be initialized as fixed, non-trainable weights in a Conv2d layer.

Sobel (H): [[-1,-2,-1], [0,0,0], [1,2,1]]

Sobel (V): [[-1,0,1], [-2,0,2], [-1,0,1]]

LoG (5x5): [[0,0,-1,0,0], [0,-1,-2,-1,0], [-1,-2,16,-2,-1], [0,-1,-2,-1,0], [0,0,-1,0,0]]

EDGE 1 (5x5): [[0,0,0,0,0],[-1,-2,0,2,1],[0,0,0,0,0],[1,2,0,-2,-1],[0,0,0,0,0]]

EDGE 2 (5x5): [[-1,0,1,0,-1],[-2,0,2,0,-2],[0,0,0,0,0],[2,0,-2,0,2],[1,0,-1,0,1]]

EDGE 3 (5x5): [[0,-1,0,1,0],[0,-2,0,2,0],[0,0,0,0,0],[0,2,0,-2,0],[0,1,0,-1,0]]

SQUARE (3x3): [[-1,2,-1], [2,-4,2], [-1,2,-1]]

SQUARE (5x5): [[-1,2,-2,2,-1],[2,-6,8,-6,2],[-2,8,-12,8,-2],[2,-6,8,-6,2],[-1,2,-2,2,-1]]

D3,3 (4x4): [[1,-3,3,-1],[-3,9,-9,3],[3,-9,9,-3],[-1,3,-3,1]]

D4,4 (5x5): [[1,-4,6,-4,1],[-4,16,-24,16,-4],[6,-24,36,-24,6],[-4,16,-24,16,-4],[1,-4,6,-4,1]]

A.2. Radial Filter Reconstruction
For a patch of size (H', W'), create a grid of integer coordinates (u, v).

Calculate the Chebyshev distance of each coordinate from the center: d = max(|u - H'/2|, |v - W'/2|).

The predicted 1D profile w(r) has num_radial_bins.

The value of the 2D filter W[u, v] is w[floor(d)].

A.3. Core Module Architectures
LayerNorm Class: Implement the LayerNorm class that supports channels_first data format, as defined in utils.py.

GRN Class: Implement the GRN (Global Response Normalization) class as defined in utils.py.

Block Class (ConvNeXt V2 Block): Implement the standard ConvNeXt V2 block as defined in convnextv2.py. It consists of a dwconv (7x7 depthwise), norm, pwconv1 (Linear), act, grn, pwconv2 (Linear), and a drop_path. Ensure permutations are handled correctly for the linear layers.

Appendix B: Detailed CrossFusionBlock Implementation
B.1. Helper Modules
create_grid_like(tensor) & normalize_grid(grid): Implement these functions to generate and normalize coordinate grids for sampling.

ContinuousPositionalBias(dim, heads): Implement this module with a small MLP to compute a bias matrix from the relative distances between query and key grids.

B.2. DeformableCrossAttention Module (for Stages 2 & 3)
__init__(self, dim, heads, ...):

Initialize to_q, to_k, to_v as nn.Conv2d(dim, inner_dim, 1).

Initialize to_offsets as a nn.Sequential block that takes q and predicts a small number of 2D offsets per query point.

Initialize rel_pos_bias as an instance of ContinuousPositionalBias.

Initialize to_out as a nn.Conv2d(inner_dim, dim, 1).

forward(self, query_map, kv_map):

Generate q from query_map and predict offsets.

Create a sampling grid vgrid by adding offsets to a base grid.

Use F.grid_sample to sample kv_feats from kv_map at the vgrid locations.

Generate k and v from kv_feats.

Compute attention scores between q and k, add the rel_pos_bias, and aggregate v.

Return the projected out feature map.

B.3. StandardCrossAttention Module (for Stages 4 & 5)
__init__(self, dim, heads, ...):

Initialize to_q, to_k, to_v as nn.Linear(dim, inner_dim).

Initialize to_out as nn.Linear(inner_dim, dim).

forward(self, query_map, kv_map):

Flatten both input maps to sequences of shape (B, H*W, C).

Generate q, k, and v using the linear layers.

Compute standard multi-head self-attention between q, k, and v.

Pass through to_out and reshape the result back to (B, C, H, W).

B.4. CrossFusionBlock Module
__init__(self, dim, use_deformable=True, ...):

Conditionally instantiate either DeformableCrossAttention or StandardCrossAttention for self.spatial_enhancer and self.frequency_enhancer.

Instantiate LayerNorm layers and ConvFFN blocks (1x1 Conv -> GELU -> 1x1 Conv) for both spatial and frequency paths.

forward(self, spatial_feat, freq_feat):

Call the enhancer modules to get enhanced_spatial and enhanced_freq.

For each path, perform the full Transformer block operation: x = norm1(x + enhanced_x), then out = norm2(x + ffn(x)).

Return the two output feature maps.





