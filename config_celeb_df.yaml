# AuraNet Configuration for Celeb-DF Dataset Training
# Dataset-specific configuration

# Dataset Configuration
dataset:
  name: "celeb_df"
  data_root: "path/to/celeb-df-dataset"  # Update this path
  subfolders:
    real: "celeb-df-real"
    fake: "celeb-df-fake" 
    mask: "celeb-df-mask"
  
  # Train/Test split configuration
  split_ratio: 0.75  # 75% train, 25% test
  sampling_strategy: "sorted_index_based"  # Sort frames within each video by frame_id, then split by index
  split_modulo: 4    # Take every 4th frame (index 3) for test, rest for train (0,1,2->train, 3->test)
  
  # Data matching strategy
  match_fake_to_mask: true  # Only use fake images that have corresponding masks

# Image processing
img_size: [256, 256]  # [H, W]

# Backbone architecture (same as base config)
num_stages: 4  # Stages 2, 3, 4, 5 of the main backbone
depths: [2, 2, 6, 2]  # Blocks per stage
dims: [64, 128, 256, 512]  # Channels per stage

# HAFT (Hierarchical Adaptive Frequency Transform) parameters
num_haft_levels: [3, 3, 2, 1]  # Hierarchical levels for HAFT in each of the 4 stages
num_radial_bins: 16
context_vector_dim: 64

# Final fusion and decoder parameters
dsf_dim: 512  # Final Fusion Dimension
decoder_embed_dim: 342
decoder_depth: 1

# Classification
num_classes: 2

# Model architecture hyperparameters
model:
  # Attention parameters
  attention_heads: 8
  attention_dropout: 0.0
  
  # MLP and FFN parameters  
  mlp_hidden_dim: 128
  ffn_hidden_ratio: 4  # FFN hidden dim = input_dim * ratio
  
  # Dropout rates
  classification_dropout: 0.15
  cross_fusion_dropout: 0.0
  
  # Reduction ratios for attention mechanisms
  se_reduction: 16
  cbam_reduction: 16
  
  # Contrastive learning
  contrastive_projection_dim: 128
  contrastive_hidden_dim: 512
  
  # AM-Softmax parameters
  am_softmax_margin: 0.35
  am_softmax_scale: 30.0
  
  # Block masking parameters
  block_size: 16  # For random masking
  
  # Initial processing channels
  msaf_srm_channels: 20
  msaf_dwt_channels: 12
  msaf_fused_channels: 32
  initial_spatial_channels: 32
  mbconv_expand_ratio: 4
  
  # HAFT internal parameters
  haft_encoder_channels: [16, 32]  # [first_conv, second_conv]
  haft_pooling_size: 4
  
  # Cross-attention offset scaling
  deformable_offset_scale: 0.1

# Data augmentation parameters
data_augmentation:
  # Common augmentations
  horizontal_flip_prob: 0.5
  rotation_degrees: 10  # Reduced for face data
  
  # Color jitter for pre-training (reduced for faces)
  brightness: 0.1
  contrast: 0.1
  saturation: 0.1
  hue: 0.05
  
  # Normalization (ImageNet stats)
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

# Multi-GPU Training Configuration
distributed:
  enabled: true
  backend: "nccl"  # Use NCCL for multi-GPU training
  init_method: "env://"
  world_size: null  # Will be set automatically
  rank: null        # Will be set automatically
  local_rank: null  # Will be set automatically
  
# Training parameters
training:
  # Pre-training stage
  pretrain:
    batch_size: 16    # Per GPU batch size
    learning_rate: 0.0001  # 1e-4
    epochs: 50        # Reduced for Celeb-DF
    mask_ratio: 0.6
    weight_decay: 0.05
    warmup_epochs: 5
    
    # Loss weights
    image_loss_weight: 1.0
    mask_loss_weight: 1.0
    supcon_loss_weight: 0.5
    
    # SSIM loss weight in mask reconstruction
    ssim_weight: 0.15
    l1_weight: 0.85
    
    # Contrastive learning
    supcon_temperature: 0.1
    
    # Early stopping
    patience: 10
    min_delta: 0.0001  # 1e-4

  # Fine-tuning stage
  finetune:
    batch_size: 8     # Per GPU batch size (smaller for fine-tuning)
    encoder_lr: 0.00001  # 1e-5, Small LR for pre-trained encoder
    head_lr: 0.001       # 1e-3, Larger LR for new heads
    epochs: 30        # Reduced for Celeb-DF
    weight_decay: 0.05
    warmup_epochs: 3
    
    # Loss weights
    seg_loss_weight: 1.0
    class_loss_weight: 1.0
    
    # SSIM loss weight in segmentation
    seg_ssim_weight: 0.15
    seg_l1_weight: 0.85
    
    # Early stopping
    patience: 8
    min_delta: 0.0001  # 1e-4

# Data loading parameters - MEMORY OPTIMIZED
data_loading:
  num_workers: 2        # Reduce from 4 to save memory
  pin_memory: false     # Disable to save GPU memory
  drop_last: true       # For training
  prefetch_factor: 1    # Reduce from 2 to save memory  
  persistent_workers: true
  
# Evaluation parameters  
evaluation:
  batch_size: 16    # Per GPU
  save_predictions: true
  visualize_results: true
  metrics: ["accuracy", "precision", "recall", "f1", "auc", "ap"]
  mask_gt_dir: "celeb-df-mask"  # Path to ground truth masks for mask evaluation (relative to data_root)
  detailed_eval_freq: 1  # Run detailed evaluation every N epochs (1 = every epoch)
  
# Checkpointing and logging
checkpoint:
  save_dir: "checkpoints/celeb_df"
  save_freq: 5  # Save every 5 epochs
  save_best: true
  resume_from: null  # Path to checkpoint to resume from
  
logging:
  log_dir: "logs/celeb_df"
  log_freq: 100  # Log every 100 steps
  tensorboard: true
  wandb:
    enabled: false  # Set to true if using Weights & Biases
    project: "auranet-celeb-df"
    entity: null    # Your wandb username/entity
    
# Hardware configuration
device: "cuda"
mixed_precision: true  # Use automatic mixed precision
compile_model: false   # Set to true for PyTorch 2.0+ compile optimization

# Reproducibility
seed: 42
deterministic: false  # Set to true for full reproducibility (slower)
