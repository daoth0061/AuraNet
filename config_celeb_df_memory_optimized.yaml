checkpoint:
  resume_from: null
  save_best: true
  save_dir: checkpoints/celeb_df
  save_freq: 5
compile_model: true
context_vector_dim: 64
data_augmentation:
  brightness: 0.1
  contrast: 0.1
  horizontal_flip_prob: 0.5
  hue: 0.05
  mean:
  - 0.485
  - 0.456
  - 0.406
  rotation_degrees: 10
  saturation: 0.1
  std:
  - 0.229
  - 0.224
  - 0.225
data_loading:
  drop_last: true
  num_workers: 4
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 2
dataset:
  data_root: path/to/celeb-df-dataset
  match_fake_to_mask: true
  name: celeb_df
  sampling_strategy: sorted_index_based
  split_modulo: 4
  split_ratio: 0.75
  subfolders:
    fake: celeb-df-fake
    mask: celeb-df-mask
    real: celeb-df-real
decoder_depth: 1
decoder_embed_dim: 342
depths:
- 2
- 2
- 6
- 2
deterministic: false
device: cuda
dims:
- 64
- 128
- 256
- 512
distributed:
  backend: nccl
  enabled: true
  init_method: env://
  local_rank: null
  rank: null
  world_size: null
dsf_dim: 512
evaluation:
  batch_size: 16
  detailed_eval_freq: 1
  mask_gt_dir: celeb-df-mask
  metrics:
  - accuracy
  - precision
  - recall
  - f1
  - auc
  - ap
  save_predictions: true
  visualize_results: true
img_size:
- 64
- 64
logging:
  log_dir: logs/celeb_df
  log_freq: 100
  tensorboard: true
  wandb:
    enabled: false
    entity: null
    project: auranet-celeb-df
memory_optimization:
  cpu_offloading: true
  flash_attention: true
  gradient_checkpointing: true  # Enable gradient checkpointing
  periodic_cache_clearing: true
  sequential_haft_processing: true
  enable_channels_last: true    # Use NHWC memory format
  enable_cudnn_autotuner: true  # Enable CuDNN autotuner
  use_optimized_modules: true   # Use optimized HAFT and CrossFusion implementations
  parallel_processing: true     # Enable parallel processing where possible
mixed_precision: true
model:
  am_softmax_margin: 0.35
  am_softmax_scale: 30.0
  attention_dropout: 0.0
  attention_heads: 8
  block_size: 16
  cbam_reduction: 16
  classification_dropout: 0.15
  contrastive_hidden_dim: 512
  contrastive_projection_dim: 128
  cross_fusion_dropout: 0.0
  deformable_offset_scale: 0.1
  ffn_hidden_ratio: 4
  haft_encoder_channels:
  - 16
  - 32
  haft_pooling_size: 4
  initial_spatial_channels: 32
  mbconv_expand_ratio: 4
  mlp_hidden_dim: 128
  msaf_dwt_channels: 12
  msaf_fused_channels: 32
  msaf_srm_channels: 20
  pretrained:
    enabled: true
    spatial_branch: "convnextv2_pico_1k_224_fcmae.pt"
    freeze_layers: []  # Empty list means no layers are frozen
    fine_tune_bn_stats: true  # Whether to fine-tune batch norm statistics
  se_reduction: 16
num_classes: 2
num_haft_levels:
- 2
- 2
- 2
- 1
num_radial_bins: 16
num_stages: 4
seed: 42
training:
  finetune:
    batch_size: 4
    class_loss_weight: 1.0
    encoder_lr: 5.0e-06  # Reduced from 1.0e-05 for pretrained weights
    epochs: 30
    head_lr: 0.001
    min_delta: 0.0001
    patience: 8
    seg_l1_weight: 0.85
    seg_loss_weight: 1.0
    seg_ssim_weight: 0.15
    warmup_epochs: 2  # Reduced from 3 for pretrained weights
    weight_decay: 0.02  # Reduced from 0.05 for pretrained weights
  gradient_accumulation_steps: 2
  pretrain:
    batch_size: 8
    epochs: 50
    image_loss_weight: 1.0
    l1_weight: 0.85
    learning_rate: 5.0e-05  # Reduced from 0.0001 for pretrained weights
    mask_loss_weight: 1.0
    mask_ratio: 0.6
    min_delta: 0.0001
    patience: 10
    ssim_weight: 0.15
    supcon_loss_weight: 0.5
    supcon_temperature: 0.1
    warmup_epochs: 3  # Reduced from 5 for pretrained weights
    weight_decay: 0.03  # Reduced from 0.05 for pretrained weights
